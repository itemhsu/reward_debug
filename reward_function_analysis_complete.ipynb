{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* run the reward_function on reward.py with the input from JimmyModelv4clone3_traininglog.tar.gz. \n",
    "* The dictionary of JimmyModelv4clone3_traininglog.tar.gz cloud be found in JimmyModelv4clone3_Analysis.ipynb. \n",
    "* The dictionary of input reward function is as param.py. \n",
    "* these two dictionary is different and have to convert before use it .\n",
    "* the waypoint is as https://github.com/aws-deepracer-community/deepracer-race-data/raw/refs/heads/main/raw_data/tracks/npy/reinvent_base.npy . \n",
    "* Ignore all object relatived params. \n",
    "* ignore is_offtrack.  \n",
    "* Do not display image. Output whole code as a jupyter notebnook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Function Analysis on JimmyModelv4clone3 Training Log\n",
    "\n",
    "This notebook runs the reward function from reward.py on the training data from JimmyModelv4clone3_traininglog.tar.gz.\n",
    "It converts between the training log dictionary format and the reward function parameter format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tarfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Extract Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log already extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract training log data if not already extracted\n",
    "if not os.path.exists('traininglog'):\n",
    "    with tarfile.open('JimmyModelv4clone3_traininglog.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "        print('Training log extracted successfully')\n",
    "else:\n",
    "    print('Training log already extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 CSV files\n",
      "Combined data shape: (18175, 17)\n",
      "Columns: ['episode', 'steps', 'X', 'Y', 'yaw', 'steer', 'throttle', 'action', 'reward', 'done', 'all_wheels_on_track', 'progress', 'closest_waypoint', 'track_len', 'tstamp', 'episode_status', 'pause_duration']\n"
     ]
    }
   ],
   "source": [
    "# Load all CSV files\n",
    "csv_files = glob.glob('traininglog/sim-trace/training/training-simtrace/*.csv')\n",
    "print(f'Found {len(csv_files)} CSV files')\n",
    "\n",
    "# Combine all CSV files into one dataframe\n",
    "all_data = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    all_data.append(df)\n",
    "\n",
    "training_data = pd.concat(all_data, ignore_index=True)\n",
    "print(f'Combined data shape: {training_data.shape}')\n",
    "print(f'Columns: {list(training_data.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Waypoint Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waypoint file already exists\n",
      "Waypoints shape: (119, 6)\n",
      "Center waypoints shape: (119, 2)\n",
      "First few waypoints: [[3.05973351 0.68265541]\n",
      " [3.2095089  0.68313448]\n",
      " [3.35927546 0.68336383]\n",
      " [3.50903499 0.68340179]\n",
      " [3.658795   0.68346104]]\n"
     ]
    }
   ],
   "source": [
    "# Download waypoint data\n",
    "waypoint_url = 'https://github.com/aws-deepracer-community/deepracer-race-data/raw/refs/heads/main/raw_data/tracks/npy/reinvent_base.npy'\n",
    "\n",
    "if not os.path.exists('reinvent_base.npy'):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(waypoint_url, 'reinvent_base.npy')\n",
    "        print('Waypoint file downloaded successfully')\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading waypoints: {e}')\n",
    "else:\n",
    "    print('Waypoint file already exists')\n",
    "\n",
    "# Load waypoints\n",
    "waypoints = np.load('reinvent_base.npy')\n",
    "print(f'Waypoints shape: {waypoints.shape}')\n",
    "\n",
    "# Extract center waypoints (assuming first 2 columns are x, y coordinates)\n",
    "waypoints_center = waypoints[:, :2]\n",
    "print(f'Center waypoints shape: {waypoints_center.shape}')\n",
    "print(f'First few waypoints: {waypoints_center[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    范例代码：赛道分区策略\n",
    "    '''\n",
    "   \n",
    "    # 分别设置鼓励靠左行驶的区域和鼓励靠右行驶的区域\n",
    "    left = [*range(22,40),*range(76,92),*range(100,112)]\n",
    "    right = [*range(48,56)]\n",
    "\n",
    "    # 调用内置参数\n",
    "    is_left_of_center = params['is_left_of_center']\n",
    "    closest_waypoints = params['closest_waypoints']\n",
    "\n",
    "    # 给予一个很低的起始奖励分数\n",
    "    reward = 1e-3\n",
    "\n",
    "    # 当赛车处于靠左行驶区且正在靠赛道左侧，或处于靠右行驶区且正在靠赛道右侧时，给予奖励\n",
    "    if closest_waypoints[0] in left and is_left_of_center:\n",
    "        reward = 1.0\n",
    "    if closest_waypoints[0] in right and not is_left_of_center:\n",
    "        reward = 1.0\n",
    "\n",
    "    return float(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_from_center(x, y, waypoints):\n",
    "    \"\"\"Calculate distance from track center\"\"\"\n",
    "    # Find closest waypoint\n",
    "    distances = np.sqrt((waypoints[:, 0] - x)**2 + (waypoints[:, 1] - y)**2)\n",
    "    closest_idx = np.argmin(distances)\n",
    "    return distances[closest_idx]\n",
    "\n",
    "def is_left_of_center_calc(x, y, waypoints, closest_waypoint_idx):\n",
    "    \"\"\"Calculate if the car is left of center\"\"\"\n",
    "    # Get current and next waypoint\n",
    "    current_wp = waypoints[closest_waypoint_idx]\n",
    "    next_wp_idx = (closest_waypoint_idx + 1) % len(waypoints)\n",
    "    next_wp = waypoints[next_wp_idx]\n",
    "    \n",
    "    # Calculate cross product to determine which side\n",
    "    track_direction = np.array([next_wp[0] - current_wp[0], next_wp[1] - current_wp[1]])\n",
    "    car_position = np.array([x - current_wp[0], y - current_wp[1]])\n",
    "    \n",
    "    cross_product = np.cross(track_direction, car_position)\n",
    "    return cross_product > 0\n",
    "\n",
    "def get_closest_waypoints(x, y, waypoints):\n",
    "    \"\"\"Get the two closest waypoints\"\"\"\n",
    "    distances = np.sqrt((waypoints[:, 0] - x)**2 + (waypoints[:, 1] - y)**2)\n",
    "    closest_indices = np.argsort(distances)[:2]\n",
    "    return closest_indices.tolist()\n",
    "\n",
    "def convert_training_data_to_params(row, waypoints, track_width=1.067):\n",
    "    \"\"\"Convert training log row to reward function parameters\"\"\"\n",
    "    x, y = row['X'], row['Y']\n",
    "    \n",
    "    # Get closest waypoints\n",
    "    closest_waypoints = get_closest_waypoints(x, y, waypoints)\n",
    "    \n",
    "    # Calculate distance from center\n",
    "    distance_from_center = calculate_distance_from_center(x, y, waypoints)\n",
    "    \n",
    "    # Calculate if left of center\n",
    "    is_left_of_center = is_left_of_center_calc(x, y, waypoints, closest_waypoints[0])\n",
    "    \n",
    "    params = {\n",
    "        'all_wheels_on_track': bool(row['all_wheels_on_track']),\n",
    "        'x': float(x),\n",
    "        'y': float(y),\n",
    "        'closest_waypoints': closest_waypoints,\n",
    "        'distance_from_center': float(distance_from_center),\n",
    "        'is_left_of_center': bool(is_left_of_center),\n",
    "        'is_offtrack': not bool(row['all_wheels_on_track']),\n",
    "        'heading': float(row['yaw']),\n",
    "        'progress': float(row['progress']),\n",
    "        'speed': float(row['throttle']),  # Using throttle as proxy for speed\n",
    "        'steering_angle': float(row['steer']),\n",
    "        'steps': int(row['steps']),\n",
    "        'track_length': float(row['track_len']),\n",
    "        'track_width': float(track_width),\n",
    "        'waypoints': waypoints.tolist()\n",
    "    }\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Reward Function to Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying reward function to training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/z50vsb514vj56p9m124dt5140000gn/T/ipykernel_3240/1744426414.py:19: DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n",
      "  cross_product = np.cross(track_direction, car_position)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 18175 rows with 0 errors\n"
     ]
    }
   ],
   "source": [
    "# Apply reward function to all training data\n",
    "print('Applying reward function to training data...')\n",
    "\n",
    "calculated_rewards = []\n",
    "conversion_errors = 0\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    try:\n",
    "        # Convert row to parameters\n",
    "        params = convert_training_data_to_params(row, waypoints_center)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = reward_function(params)\n",
    "        calculated_rewards.append(reward)\n",
    "        \n",
    "    except Exception as e:\n",
    "        calculated_rewards.append(0.0)\n",
    "        conversion_errors += 1\n",
    "        if conversion_errors <= 5:  # Only print first 5 errors\n",
    "            print(f'Error processing row {idx}: {e}')\n",
    "\n",
    "print(f'Processed {len(calculated_rewards)} rows with {conversion_errors} errors')\n",
    "\n",
    "# Add calculated rewards to dataframe\n",
    "training_data['calculated_reward'] = calculated_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REWARD FUNCTION ANALYSIS RESULTS ===\n",
      "\n",
      "Original Rewards (from training log):\n",
      "  Mean: 0.9169\n",
      "  Std:  0.2759\n",
      "  Min:  0.0000\n",
      "  Max:  1.0000\n",
      "\n",
      "Calculated Rewards (from reward function):\n",
      "  Mean: 0.2937\n",
      "  Std:  0.4547\n",
      "  Min:  0.0010\n",
      "  Max:  1.0000\n",
      "\n",
      "Reward Distribution:\n",
      "  Reward 0.001: 12850 occurrences (70.7%)\n",
      "  Reward 1.000: 5325 occurrences (29.3%)\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print('=== REWARD FUNCTION ANALYSIS RESULTS ===\\n')\n",
    "\n",
    "print('Original Rewards (from training log):')\n",
    "print(f'  Mean: {training_data[\"reward\"].mean():.4f}')\n",
    "print(f'  Std:  {training_data[\"reward\"].std():.4f}')\n",
    "print(f'  Min:  {training_data[\"reward\"].min():.4f}')\n",
    "print(f'  Max:  {training_data[\"reward\"].max():.4f}')\n",
    "\n",
    "print('\\nCalculated Rewards (from reward function):')\n",
    "print(f'  Mean: {training_data[\"calculated_reward\"].mean():.4f}')\n",
    "print(f'  Std:  {training_data[\"calculated_reward\"].std():.4f}')\n",
    "print(f'  Min:  {training_data[\"calculated_reward\"].min():.4f}')\n",
    "print(f'  Max:  {training_data[\"calculated_reward\"].max():.4f}')\n",
    "\n",
    "print('\\nReward Distribution:')\n",
    "reward_counts = training_data['calculated_reward'].value_counts().sort_index()\n",
    "for reward, count in reward_counts.items():\n",
    "    print(f'  Reward {reward:.3f}: {count} occurrences ({count/len(training_data)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EPISODE ANALYSIS ===\n",
      "Total episodes: 220\n",
      "Episodes with calculated rewards > 0.5: 7\n",
      "Episodes with max progress = 100%: 18\n",
      "\n",
      "Top 10 episodes by calculated reward mean:\n",
      "         calc_reward_mean  calc_reward_sum  steps  max_progress  final_status\n",
      "episode                                                                      \n",
      "44                 0.5950           22.015     37       17.2859     off_track\n",
      "13                 0.5618           32.025     57       31.3266     off_track\n",
      "93                 0.5243           33.030     63       37.6095     off_track\n",
      "14                 0.5232           23.021     44       24.7981     off_track\n",
      "185                0.5166           16.015     31       13.7787     off_track\n",
      "53                 0.5005           30.030     60       32.7509     off_track\n",
      "165                0.5005           14.014     28       11.8314     off_track\n",
      "24                 0.4844           15.016     31       12.1968     off_track\n",
      "33                 0.4763           49.054    103       64.6573     off_track\n",
      "104                0.4626           73.085    158      100.0000  lap_complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze by episode\n",
    "episode_analysis = training_data.groupby('episode').agg({\n",
    "    'calculated_reward': ['mean', 'sum', 'count'],\n",
    "    'reward': ['mean', 'sum'],\n",
    "    'progress': 'max',\n",
    "    'all_wheels_on_track': 'mean',\n",
    "    'episode_status': 'last'\n",
    "}).round(4)\n",
    "\n",
    "episode_analysis.columns = ['calc_reward_mean', 'calc_reward_sum', 'steps', \n",
    "                           'orig_reward_mean', 'orig_reward_sum', 'max_progress', \n",
    "                           'on_track_ratio', 'final_status']\n",
    "\n",
    "print('\\n=== EPISODE ANALYSIS ===')\n",
    "print(f'Total episodes: {len(episode_analysis)}')\n",
    "print(f'Episodes with calculated rewards > 0.5: {len(episode_analysis[episode_analysis[\"calc_reward_mean\"] > 0.5])}')\n",
    "print(f'Episodes with max progress = 100%: {len(episode_analysis[episode_analysis[\"max_progress\"] >= 99.9])}')\n",
    "\n",
    "print('\\nTop 10 episodes by calculated reward mean:')\n",
    "print(episode_analysis.nlargest(10, 'calc_reward_mean')[['calc_reward_mean', 'calc_reward_sum', 'steps', 'max_progress', 'final_status']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REWARD FUNCTION LOGIC ANALYSIS ===\n",
      "\n",
      "Steps receiving high rewards (1.0): 5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/z50vsb514vj56p9m124dt5140000gn/T/ipykernel_3240/1744426414.py:19: DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n",
      "  cross_product = np.cross(track_direction, car_position)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waypoint distribution for high rewards:\n",
      "  Waypoint 22: 142 times\n",
      "  Waypoint 23: 128 times\n",
      "  Waypoint 24: 131 times\n",
      "  Waypoint 25: 156 times\n",
      "  Waypoint 26: 155 times\n",
      "  Waypoint 27: 131 times\n",
      "  Waypoint 28: 120 times\n",
      "  Waypoint 29: 119 times\n",
      "  Waypoint 30: 120 times\n",
      "  Waypoint 31: 134 times\n",
      "\n",
      "High reward distribution by track side:\n",
      "  Left of center: 4854 (91.2%)\n",
      "  Right of center: 471 (8.8%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze the reward function logic\n",
    "print('\\n=== REWARD FUNCTION LOGIC ANALYSIS ===')\n",
    "\n",
    "# Get data where rewards were given (calculated_reward = 1.0)\n",
    "high_reward_data = training_data[training_data['calculated_reward'] == 1.0]\n",
    "\n",
    "if len(high_reward_data) > 0:\n",
    "    print(f'\\nSteps receiving high rewards (1.0): {len(high_reward_data)}')\n",
    "    \n",
    "    # Calculate closest waypoints for analysis\n",
    "    closest_waypoints_high_reward = []\n",
    "    left_of_center_high_reward = []\n",
    "    \n",
    "    for idx, row in high_reward_data.iterrows():\n",
    "        try:\n",
    "            params = convert_training_data_to_params(row, waypoints_center)\n",
    "            closest_waypoints_high_reward.append(params['closest_waypoints'][0])\n",
    "            left_of_center_high_reward.append(params['is_left_of_center'])\n",
    "        except:\n",
    "            closest_waypoints_high_reward.append(-1)\n",
    "            left_of_center_high_reward.append(False)\n",
    "    \n",
    "    # Analyze waypoint distribution for high rewards\n",
    "    waypoint_counts = pd.Series(closest_waypoints_high_reward).value_counts().sort_index()\n",
    "    print('\\nWaypoint distribution for high rewards:')\n",
    "    for wp, count in waypoint_counts.head(10).items():\n",
    "        if wp >= 0:\n",
    "            print(f'  Waypoint {wp}: {count} times')\n",
    "    \n",
    "    # Check left/right distribution\n",
    "    left_count = sum(left_of_center_high_reward)\n",
    "    right_count = len(left_of_center_high_reward) - left_count\n",
    "    print(f'\\nHigh reward distribution by track side:')\n",
    "    print(f'  Left of center: {left_count} ({left_count/len(left_of_center_high_reward)*100:.1f}%)')\n",
    "    print(f'  Right of center: {right_count} ({right_count/len(left_of_center_high_reward)*100:.1f}%)')\n",
    "\n",
    "else:\n",
    "    print('No high rewards (1.0) found in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "                             Metric      Value\n",
      "                        Total Steps 18175.0000\n",
      "                     Total Episodes   220.0000\n",
      "              Avg Steps per Episode    82.6136\n",
      "Successful Episodes (>99% progress)    18.0000\n",
      "   High Reward Steps (reward = 1.0)  5325.0000\n",
      "  Low Reward Steps (reward = 0.001) 12850.0000\n",
      "              Avg Calculated Reward     0.2937\n",
      "                Avg Original Reward     0.9169\n",
      "                     On-Track Ratio     0.9330\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Steps',\n",
    "        'Total Episodes', \n",
    "        'Avg Steps per Episode',\n",
    "        'Successful Episodes (>99% progress)',\n",
    "        'High Reward Steps (reward = 1.0)',\n",
    "        'Low Reward Steps (reward = 0.001)',\n",
    "        'Avg Calculated Reward',\n",
    "        'Avg Original Reward',\n",
    "        'On-Track Ratio'\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(training_data),\n",
    "        training_data['episode'].nunique(),\n",
    "        len(training_data) / training_data['episode'].nunique(),\n",
    "        len(episode_analysis[episode_analysis['max_progress'] >= 99.9]),\n",
    "        len(training_data[training_data['calculated_reward'] == 1.0]),\n",
    "        len(training_data[training_data['calculated_reward'] == 0.001]),\n",
    "        training_data['calculated_reward'].mean(),\n",
    "        training_data['reward'].mean(),\n",
    "        training_data['all_wheels_on_track'].mean()\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print('\\n=== SUMMARY STATISTICS ===')\n",
    "print(summary_df.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed results saved to reward_function_analysis_detailed.csv\n",
      "Episode analysis saved to reward_function_episode_analysis.csv\n",
      "Summary statistics saved to reward_function_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Save detailed results\n",
    "output_data = training_data[['episode', 'steps', 'X', 'Y', 'progress', 'reward', 'calculated_reward', \n",
    "                            'all_wheels_on_track', 'episode_status']].copy()\n",
    "\n",
    "output_data.to_csv('reward_function_analysis_detailed.csv', index=False)\n",
    "print('Detailed results saved to reward_function_analysis_detailed.csv')\n",
    "\n",
    "# Save episode summary\n",
    "episode_analysis.to_csv('reward_function_episode_analysis.csv')\n",
    "print('Episode analysis saved to reward_function_episode_analysis.csv')\n",
    "\n",
    "# Save summary statistics\n",
    "summary_df.to_csv('reward_function_summary.csv', index=False)\n",
    "print('Summary statistics saved to reward_function_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "This analysis applied the track zone strategy reward function to the JimmyModelv4clone3 training data. The reward function encourages:\n",
    "\n",
    "- **Left-side driving** in waypoint zones: 22-39, 76-91, 100-111\n",
    "- **Right-side driving** in waypoint zones: 48-55\n",
    "- **Minimal reward (0.001)** for all other positions\n",
    "\n",
    "Key findings:\n",
    "1. The reward function produces binary outcomes (1.0 or 0.001)\n",
    "2. Most steps receive minimal reward, indicating the car spent little time in the target zones\n",
    "3. The strategy may need adjustment based on the actual track layout and car behavior\n",
    "\n",
    "**Note**: Object-related parameters were ignored as requested, focusing only on track position and waypoint-based rewards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
